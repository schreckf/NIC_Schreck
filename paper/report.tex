% TeX-Engine = pdflatex
% !TeX program = lualatex
\documentclass[12pt]{article}


% Packages
% ========
\usepackage{mathtools, amssymb}

\usepackage{amsmath} 

%\usepackage[math-style=TeX,bold-style=upright]{unicode-math}
%\setmathfont[Ligatures=TeX]{XITS Math}

%\usepackage{fontspec}
%\setmainfont[Ligatures=TeX]{Lato}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

%\usepackage{polyglossia}
%\setdefaultlanguage{english}
\usepackage[english]{babel}



\usepackage[a4paper, top=1in, bottom=1.25in, left=1in, right=1in]{geometry}
\usepackage[onehalfspacing]{setspace}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[L]{}
\fancyhead[C]{}
\fancyhead[R]{\nouppercase{\leftmark}}
\fancyfoot[L]{}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{}

\usepackage[labelfont=bf, font=small,justification=justified, singlelinecheck=false]{caption}
\usepackage{booktabs}
\usepackage{multirow}


\usepackage{graphicx}
%\usepackage{float}
%\usepackage{subfig}

\usepackage{tabularx,ragged2e}
%\newcolumntype{C}{>{\centering\arraybackslash}X}

\usepackage{url} % nicer url in references

\usepackage{wrapfig} %necessary to wrap figures

\usepackage{flafter}

\usepackage[svgnames,table,xcdraw]{xcolor}

\usepackage{listings}
\lstset{    
    language        =R,
    backgroundcolor =\color{GhostWhite},
    basicstyle      =\ttfamily,
    breaklines      =true,
    commentstyle    =\color{Gray},
    keepspaces      =false,
    keywordstyle    =\color{black},
    deletekeywords  ={data,_},
    morekeywords    ={},
    numbers         =left,
    numbersep       =8pt,
    numberstyle     =\tiny,
    rulecolor       =\color{black},
    showstringspaces=false,
    showtabs        =false,
    showstringspaces=false,
    stringstyle     =\color{black},
    tabsize         =2,
    frame           =single
}

\usepackage[title]{appendix}
\usepackage{enumitem}

\usepackage[most]{tcolorbox}
%\newtcolorbox{mthm}{breakable, enhanced, sharp corners, colframe=black, colback=GhostWhite, coltitle=black}

\usepackage{tikz}
\usetikzlibrary{positioning,shadows}

%\usepackage{filecontents}

%\usepackage[backend=biber, minbibnames=3, bibstyle=authoryear, citestyle=authoryear, natbib, sorting=nyt]{biblatex}
%\DefineBibliographyStrings{english}{andothers = {et\addabbrvspace al\adddot}}
%\addbibresource{report.bib}
\usepackage[round]{natbib}

%\usepackage[unicode,colorlinks,filecolor=DarkGreen,citecolor=DarkRed,linkcolor=black,urlcolor=DarkBlue]{hyperref}
\usepackage[colorlinks, filecolor=magenta, citecolor=DarkBlue, linkcolor=black, urlcolor=blue]{hyperref}

\usepackage{wrapfig}

%\usepackage{icomma}

%\newcommand{\Quantlet}[2]{\includegraphics[scale = 0.05]{q.pdf}\,\href{#2}{#1}}

\title{Stacking algorithm and ensemble modeling}
\author{Frederik Schreck}
\date{Berlin, \today}


\renewcommand{\familydefault}{\rmdefault} % is computer modern sans serif

\begin{document}


%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{titlepage} % Suppresses displaying the page number on the title page and the subsequent page counts as page 1
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for horizontal lines, change thickness here
	

	%------------------------------------------------
	%	Headings
	%------------------------------------------------

	\begin{minipage}{0.77\textwidth}
		\begin{flushleft}
			\large
			\textsc{Humboldt University of Berlin\\
			Course: Numerical Introductory Seminar\\
			Supervisor: Prof. Dr. Brenda López Cabrera\\}
		\end{flushleft}
	\end{minipage}
		~
	\begin{minipage}{0.2\textwidth}
		\begin{flushleft}
		\includegraphics[width=0.73\textwidth]{graphs/HU_logo.png}\\
		\end{flushleft}
	\end{minipage}
	
	\vspace{2cm}

	%------------------------------------------------
	%	Title
	%------------------------------------------------
	\center
	\HRule\\[0.4cm]
	
	{\LARGE\bfseries Stacking Algorithm for Ensemble Modelling}\\[0.05cm] % Title of your document
		\HRule\\[1.5cm]
	
	%------------------------------------------------
	%	Author(s)
	%------------------------------------------------
			
			\textit{Submitted by}\\	
			\vspace{0.5cm}
			

	\begin{minipage}{0.5\textwidth}
		\begin{center}
			\large

			\textsc{Frederik Schreck \\
			Statistics M.Sc.\\
			Humboldt University\\
			580567} % Your name
		\end{center}

	\end{minipage}
	

	\vfill\vfill\vfill\vfill % Position the date 3/4 down the remaining page
	
	{\large\today} % Date, change the \today to a set date if you want to be precise
	
	\vfill % Push the date up 1/4 of the remaining page
	
\end{titlepage}

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\tableofcontents
\thispagestyle{empty}
\clearpage
\setcounter{page}{1}
\newpage

\listoffigures
\listoftables
\thispagestyle{empty}
\clearpage
\setcounter{page}{1}


%%
%%
%%
%%

\section{Abstract}

\section{Motivation}\label{Intro}
"[W]hen our imperfect judgments are aggregated in the right way, our collective intelligence is often excellent."\citep[Foreword p.XIV]{surowiecki2005wisdom}\\

In accordance with the title of his book, Surowiecki refers to what he calls the \textit{wisdom of crowds}-phenomenon \cite{surowiecki2005wisdom}. He refers to the social phenomenon, that - under certain criteria - the aggregates of individual judgements can be superior to each individual judgement. While this effect can be found in the social world, it also applies to the world of statistics and machine learning. In the field of Stacking and Ensemble Modelling, research has shown different ways in which the aggregation of predictive models can deliver a more powerful model. Such Stacking and Ensemble Models currently belong to the most powerful machine learning tools and win many data science competitions (e.g. on the data science competition website \citeauthor{kaggle}). Regarding the diversity of different approaches, this paper aims to discuss and apply the most established ones in a financial application of credit risk assessment. In depth, the concept of Bagging and the Random Forest model \citep{breiman1996bagging, breiman2001random}, the concept of boosting and the Gradient Boosting model \cite{freund1996experiments, friedman2002stochastic} as well as the idea of Stacked Generalization \citep{wolpert1992stacked} will be focused on. [!motivating examples: list some financial application, where ensembling brought a big benefit, e.g. saved many costs.]

The paper's structure is as follows: In the next section, the stacking and ensemble methods shall be introduced and reviewed with regards to the current state of research. Subsequently, the motivation and benefits of applying these models in the context of credit risk classification are discussed. In the following, the methodology for the practical evaluation of the models in such context is prepared.  For that, firstly the credit risk data is presented, secondly the model building process is outlined and thirdly the metrics for model evaluation are introduced. In the next section, results of the empirical analysis are presented in detail. Finally, conclusions about comparative advantages and shortfalls of the models in the context of credit risk classification are drawn and needs of further research are identified.

%The paper in which Francis Galton describes his findings on the wisdom of the crowd:
%Vox Populi. Nature 75, pages 450-451, 1907. 	
%http://galton.org/essays/1900-1911/galton-1907-vox-populi.pdf

\section{Literature review}
Ensemble learning generally refers to the combination of multiple hypotheses in order to obtain a more powerful hypothesis. In the context of machine learning, the term \textit{hypothesis} refers to the output of an algorithm, which aims to learn a target function $f(\mathbf{x})$ by using a set the features $\mathbf{x}$. Each algorithm that is used in the combination process of an ensemble learner is called a \textit{base learner}.

Formally this means, that given training data $D^{train} := \{\mathbf{x}, y\} = \{(x_1, y_1), (x_2, y_2),..., (x_N, y_N)\}$, where $x_i = (x_{i1}, x_{i2},..., x_{iK})$ is the vector of feature values for each observational unit $i \in \{1, 2,..., N\}$ and $y = f(\mathbf{x})$ is the target vector that can be modelled by an unknown function $f$ of the features. Further let $K$ denote the number of features and $N$ denote the number of observations. A set of base learners of size $M$ delivers hypotheses $h_1, h_2,..., h_M \in H$ about the true function $f$, whereby $H$ denotes the hypothesis space.

Different ways to combine the hypotheses of base learners exist. Generally, ensemble learning is most effective when combining diverse base learners. Hereby, diversity refers to error diversity, implying that the different base learners have different strengths in capturing structure in the data. \cite{brown2005diversity} showed that the combination methods of ensemble learning strategies enhance such diversity. Practical applications of ensembling techniques have shown, that stacking and ensembling models enhance diversity amongst their base learners by requiring different base learner algorithms, different hyperparameter settings, different feature subsets and different training sets \citep{online2017stacking}. [MORE RESEARCH]

In the following, different techniques to combine the hypotheses of base learners will be introduced. Furthermore, the current state of research is reviewed. 

\subsection{Bagging and the Random Forest model}
The idea of Bagging was originally proposed by \cite{breiman1996bagging}. The term abbreviates bootstrap aggregating and refers to a manipulation of the training data: Each base learner $m$ is fitted using a random sample $D^{train}_m$ that is uniformly drawn from the training data $D^{train}$. Since sampling is done with replacement, $D^{train}_m$ may contain duplicates of certain observational units. The hypotheses of the base learners is then aggregated by averaging in case of a regression problems or majority voting in case of a classification problem. 

In so far, bagging is a meta-algorithm that can be used with every type of base learner models. However, especially unstable base learners that are sensitive to data manipulation should be combined \citep[p.124]{breiman1996bagging}. Breiman therefore recommends to use Neural Networks, Decision Trees	or subset selection in Linear Regression. By building the base learners on different subsets of the data, the bagging procedure enhances diversity amongst them and can lead to "substantial gains in accuracy" \citep[p.123]{breiman1996bagging}.

The Random Forest model uses the bagging principle and supplements it by the random subspace approach \citep{ho1998random, breiman2001random}. This approach builds each base learner on a random sample with replacement of all available features. This implies a decorrelation of the hypotheses of the set of base learners used for ensembling. For Random Forests, Decision Trees are the preferred base learner algorithm \citep[cp.][]{breiman2001random}.

A big strength of the Random Forest is the reduction in prediction variance compared to single Decision Trees, which stems from the diversification. Clearly, the computational costs of a Random Forest can be much higher than of a single Tree, since computational time increases linearly to the number of consulted Trees. Due to the independent building of individual base learners, Random Forest model building can be speeded up by parallelization on different cores of the computer. Breiman further notes that Random Forests give "useful internal estimates of error, strength, correlation and variable importance" \citep[p.10]{breiman2001random}. Hence, even though decision rules of Random Forests are less transparent than those of single Decision Trees due to the sampling, relative variable importances can be calculated as a by-product by randomly permuting features and examining the influence on the prediction. As a single weakness, growing a large random forest can be computationally very expensive. Breiman proved that the generalization error of the ensemble converges almost surely to a limit with increasing number of Trees \citep[p.30]{breiman2001random}. In practice, the number of Trees is however restricted by the amount of available computational resources.

% Benchmark studies OR/AND Weiterentwicklungen des Random Forest??
%successfull applications:(Bauer&Kohavi, 1999; Drucker et al., 1994; Schapire, 1999).

\subsection{Boosting and the Gradient Boosting model}
Beneath Bagging, another powerful ensembling technique is Boosting. Boosting builds on the idea that the aggregation of simple base learners may lead to a strong learner. In the so-called Adaboost algorithm, Freund and Shapire (\citeyear{freund1996experiments}) start with an ensemble of one weak learner and iteratively add one more weak learner that aims to correct for the (pseudo) residuals of the current ensemble. Thereby, the calculation of these residuals is based on an iterative reweighting of the data. The weights of each datapoint $\mathbf{x_{i}}$ for model $m$ depend on the prediction accuracy of the current ensemble hypothesis for that datapoint $h^{ensemble}_{\{1,2,...,m-1\}}(\mathbf{x_{i}})$. 

With Boosting, it is possible to decrease the training error to zero \citep[p.11ff.]{freund1996experiments}. Furthermore, as long as base learners are better than random guessing, the Boosting technique is also able to reduce the generalization error independent of the base learning algorithm.

A development of the Boosting idea is the (Stochastic) Gradient Boosting model, that is currently the most commonly used Boosting model \citep{friedman2001greedy, friedman2002stochastic}. In contrast to the data weighting scheme in the Adaboost algorithm, Gradient Boosting minimizes the gradient of a loss function of the error by applying gradient descent. Typically, small Decision Trees are used as base learners of the Gradient Boosting model due to their propensity towards high prediction bias. Additionally, Gradient Boosting integrates the bagging idea. Friedman was able to show, that this integration could substantially improve accuracy and execution speed of the model \citep{friedman2002stochastic}.

The possibility to be executed fastly is a reason for the heavy use of Gradient Boosting models for machine learning problems. Furthermore, they allow to gain insights into the dependence of output and features by enabling partial dependence plots \citep[p.1219ff.]{friedman2001greedy}. However, due to its nature, Gradient Boosting models are highly prone to overfit and therefore must be accompanied with regularization methods \citep[p.1203]{friedman2002stochastic}.

% Benchmark studies OR/AND Weiterentwicklungen des Gradient Boosting??

\subsection{Stacked Generalization model}\label{stacking}
Stacked Generalization models has been introduced by Wolpert \citeyear{wolpert1992stacked} and defines a way to combine multiple predictive algorithms by using a second-level algorithm. In contrast to Bagging or Boosting, Stacked Generalization is typically applied to a space of different base learning models. The idea is, that different kinds of models that are applied to the learning problem are able to capture only part of the problem. Combining models with different strengths in the right way would then lead to improved predictive accuracy. Stacked Generalization is therefore also referred to as a second-stage model.

The Stacking algorithm involves partitioning of training dataset $D^{train} = (\mathbf{x}, y)$ into $J = \{1, 2,..., J\}$ disjoint parts $D^{train}_1, D^{train}_2,..., D^{train}_J$. For each of these subsets $D^{train}_j$, called "level 0 learning set", a base learner $m \in \{1,2,...M\}$, also referred to as "level 0 generalizer", is built on behalf of the training dataset without this subset $D^{train}_{-j}$ \citep[cp.]{wolpert1992stacked}. In each of the iterations, the model built on $D^{train}_{-j}$ is used to predict on subset $D^{train}_j$. The predictions of the $J$ subsets are then combined again in order to obtain a prediction of the whole training dataset $D^{train}$. Further, each level 0 generalizer is used to predict on the test dataset $D^{test}$. Due to the level 0 generalizers being built $J$ times on different disjoint subsets of the training data, Stacked Generalization can be seen as a sophisticated form of cross-validation. The next step is building a meta learner, referred to as "level 1 generalizer", that produces a prediction by using the training dataset predictions of the $M$ level 0 generalizers as inputs. Wolpert could show that this stacking procedure is able to reduce the bias of the base learners and thus minimizes the generalization error rate. He even recommends to use a version of Stacked Generalization in any real-world problem \citep[p.2]{wolpert1992stacked}. 

Different meta learning algorithms can be used for the combination of base learners. An optimal meta algorithm finds the best way to use the strengths of the base learners. Overfitting problem is especially present in Stacked Generalization models. This is due to the base learners all predicting the same target \citep{online2017stacking}. As a consequence, cross-validation and regularization can be used. Further more, the chosen meta learning algorithm should not be sensible to collinearity. It is therefore especially recommended to use Random Forests, Regularized Regression, Gradient Boosting or hill climbing methods \citep{SASpaper}. 






\section{Ensemble modelling in Credit Risk Classification}
Techniques of Stacking and Ensemble Learning are applied to predictive problems of a broad range of topics. This paper will especially focus on the application of Ensemble Learning in credit risk classification problems. Credit risk assessment and especially the its modelling is an important part in the field of financial risk management since for most small- and medium-sized banks, interests on loans are still the primary financial source \citep[p.2]{jacobson2006internal}. The banking supervision accord Basel II, that was published in 2004 and applies to member states of the European Union since 2007, restricted the buffer capital on banks and therefore makes it especially important for them to estimate the riskiness of loan applicants \citep{basel2}. For that, banks need to be able to distinguish between applicants into risky and non-risky applicants. Two opposing factors determine the banks' business rules regarding loans: On the one hand, more loans are better. On the other hand, a bank can not afford to make to many bad loans, since this would eventually lead to a collapse of the bank. A good strategy on loans will therefore be a compromise. 

Applying Ensemble Learning techniques to credit risk modelling has already proven to be highly valuable. Zhu et al. \citeyear{zhu2017comparison} investigate credit risk assessment for small- and medium-sized Chinese enterprises. For that, they carry out an experiment in which they compare the predictive performance of individual machine learning methods and ensembling methods of different complexity. They find especially the more complex ensembling methods to be of outstanding discriminative accuracy \citep[p.46f.]{zhu2017comparison}. Yu et al. \citeyear{yu2008credit} succesfully apply an ensemble learner comprising six levels of stacked Neural Networks in order to evaluate credit risk at the measurement level. Hereby, they further incorporate the Bagging approach. They conclude, that such technique "provides a promising solution to credit risk analysis" \citep[p.1443]{yu2008credit}. [MAYBE SOME MORE STUDIES]




\section{Methodology}
In order to evaluate and compare the introduced stacking and ensemble models, an empirical evaluation study is conducted. The quantlets for replication of the study can be accessed in the corresponding \href{https://github.com/schreckf/NIC_Schreck}{\color{DarkBlue}{github repository}}. In this section, the dataset used for the evaluation study is presented, the model building process is explained in detail and the metrics for evaluation are introduced.

\subsection{Data description}
In order to evaluate the introduced Stacking and Ensemble models, the German Credit Dataset from the UCI machine learning repository is used \citep{dataset}. The dataset classifies people as either being good or bad customers for a bank with respect to credit risk. It comprises a total number of 1000 observations and 20 features. Tables \ref{summary_numeric} and \ref{summary_categorical} in the Appendix present the summary statistics for the numerical and the categorical features in the original dataset. To ensure better model performance, the numerical data is later standardized before model building. The dataset is partitioned in a training dataset, comprising 750 observations, and a test dataset, comprising 250 observations.





\subsection{Model building process}
The model building process consists of feature selection, training , tuning and prediction. For the purpose of this study, an extensive set
of models goes through this process: A Random Forest model and a Gradient Boosting model represent the Ensemble Learners. For the Stacked Generalization models, the Random Forest model and the Gradient Boosting model are rebuilt as level 0 generalizers. Additionally, a Decision Tree, a Neural Network as well as a Logistic Regression model are built in order to provide a diverse set of level 0 generalizers for the Stacked Generalization models. Four different such Stacking models are built by using different subsets of base learners' predictions. Stacking model 1 and 2 use all level 0 predictions versus a the three best level 0 predictions, respectively, and combine them via averaging. Stacking model 3 and 4 both use all level 0 predictions and combine them via a Gradient Boosting model and a Logistic Regression model, respectively. All models deliver probabilistic predictions. In order to speed up the model building, all models are constructed by use of parallelization across the cores of the computer. 

Before training the model, feature selection is a critical step. The aim of feature selection is dimension reduction. Building the models on this subset may reduce their training time, reduce the variance and make the model more easily interpretable \citep{guyon2003introduction}. Since, the optimal subset of features depends on each model, a wrapper approach for feature selection is applied to each model specifically in form of wrapper approaches. Each model-specific wrapper approach starts with building the model on an intercept model and sequentially adds the next best feature (sequential foreward selection). The wrapper approach stops when no further feature can be added that increases the AUC measure (see section \ref{metrics}) by at least 0.00001 units. All wrapper approaches are run on 3-fold cross-validation in order to avoid overfitting problems (A larger number of folds leads to instabilities due to the small sample size). The subset of features identified by the model-specific wrapper approaches is subsequently used for training of the corresponding models. Since the Random Forest and the Gradient Boosting models are built as Ensemble Models and as level 0 generalizer for the Stacked Generalization models, independent wrapper approaches are applied for both versions.

The training process for each model generally consists of establishing a broad-grid tuning of all relevant hyperparameters on the training dataset in order to find the (locally) optimal parameter choices. In order to avoid overfitting on the training dataset, each combination of hyperparameters is tested by a 3-fold cross-validation process. For the Random Forest and the Gradient Boosting model, their tuned versions can directly be used for prediction on the test dataset.

For the Stacked Generalization models, the training dataset is partitioned into five disjoint subsets. Each of the five level 0 generalizers is then build in five iterations as described in section \ref{stacking}. Again, each iteration involves a parameter tuning on 3-fold cross-validation. All level 0 generalizers are then used to predict the observations in the test dataset. Stacking model 1 is then built by averaging over the probabilistic predictions of all five level 0 generalizers. Stacking model 2 is constructed by averaging over the probabilistic predictions of the three least correlated predictions of level 0 generalizers. For that, the correlations of the level 0 generalizers on the training data are investigated. Stacking model 3 is built by using again all level 0 generalizers and a Random Forest as a combining model. The parameters of the combining model are again tuned under 3-fold cross-validation. Finally, Stacking model 4 is constructed by combining the three least correlated predictions of level 0 generalizers by using a Random Forest combiner.

\subsection{Evaluation metrics}\label{metrics}
In credit risk modelling, the misclassification costs are often type-specific. This means that a false negative prediction may have different costs for the bank than a false positive prediction. When misclassification costs are known, a cost-sensitive model building strategy should be consulted. Since in the context of this paper misclassification costs are however unknown, equal misclassification costs for false negative and false positive predictions are assumed. Furthermore, the broad field of cost-sensitive learning may serve as a topic for other studies. 

\paragraph{AUC:} In the model building process, tuning of model parameters for each model $m$ is evaluated on the Area Under Curve metric (AUC). The AUC is a ranking indicator that measures the area under the receiver operating characteristic curve (ROC curve) \citep{hanley1982meaning}. For a probabilistic prediction, like in the context of this study, a visualization of the ROC curve can be obtained by plotting the sensitivity against $1 -$ specificity for all cut-off thresholds between zero and one. The AUC value can therefore take values between zero and one as well. A random model would obtain an AUC value of $0.5$, which can thus function as a benchmark value in model evaluation on AUC. In a statistical sense, the AUC estimates the probability that a randomly chosen correct prediction is correctly ranked higher than a randomly chosen false prediction. Even for probabilistic predictions, the AUC generalizes over all possible cut-off thresholds. For model $m$ can be calculated as
\begin{equation}
AUC^m = \frac{1}{P \times N}\sum_{j=1}^{P}\sum_{k=1}^{N}(\hat{y}^m_j - \hat{y}^m_k)
\end{equation}
, whereby $P$ and $N$ denote the positive (in our case "good") and negative ("bad") instances amongst the outcome values in the credit data. Further, $\hat{y}^m_j$ and $\hat{y}^m_k$ are the predictions for the positive instance $y_j$ and the prediction for the negative instance $y_k$, respectively.

\paragraph{Accuracy:} A further important metric in evaluation of classification models is the Accuracy metric, which can be interpreted as the percentage of correctly classified points. Therefore, the Accuracy metric uses a cut-off threshold of $0.5$ for probabilistic prediction, which may be used to contrast it against the AUC metric.
\begin{equation}
Accuracy^m = \frac{TP^m + TN^m}{FP^m + FN^m + TP^m + TN^m}
\end{equation}
, whereby $TP^m$ is the number of true positive predictions for model $m$ and $TN^m$, $FP^m$ and $FN^m$ are the corresponding number of true negatives, the number of false positives and the number of false negatives for model $m$, respectively.

\paragraph{Logarithmic Loss:} The Logarithmic Loss is a metric for evaluating class predictions that penalizes for a high confidence about incorrect classifications. For the case of a binary outcome, the Logarithmic Loss is given by\\
\begin{equation}
LogLoss^m = - \frac{1}{N}\sum_{i=1}^{N}(y^m_i\log(p^m_i) + (1 - y^m_i)\log(1 - p^m_i))
\end{equation}
, whereby $p^m_i$ is model $m$'s prediction for observation $y_i$.

\paragraph{Brier Score:} The models will also be assessed on the Brier Score, which is identical to the Mean Squared Error metric in statistics.
\begin{equation}
Brier^m = \frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y}^m_i)^2
\end{equation}
, whereby $\hat{y}^m_i$ denotes the predicted probability of model $m$ for observation $y_i$. 


\section{Results}\label{results}
[maybe include: BiasVarianceTradeoff-graph, AUCplot with all models in]
In the following, the results of the empirical application of Stacking and Ensemble models on a credit risk classification problem will be presented. Table \ref{eval} shows the evaluations of all models on the test dataset with respect to the metrics AUC, Accuracy, Logarithmic Loss and Brier Score. 

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & AUC & Accuracy & Logarithmic Loss & Brier Score \\ 
  \hline
Random Forest & 0.78 & 0.73 & 0.52 & 0.18 \\ 
  Gradient Boosting & 0.78 & 0.72 & 0.52 & 0.18 \\
\hline 
  Decision Tree (level 0) & 0.69 & 0.65 & 0.60 & 0.21 \\ 
  Logit Regression (level 0) & 0.71 & 0.74 & 0.57 & 0.19 \\ 
  Neural Network (level 0) & 0.76 & 0.74 & 0.53 & 0.18 \\ 
  Random Forest (level 0) & 0.73 & 0.69 & 0.56 & 0.19 \\ 
  Gradient Boosting (level 0) & 0.80 & 0.74 & 0.52 & 0.17 \\ 
\hline
  Stacking Model 1 (average, all) & 0.77 & 0.65 & 0.24 & 0.18 \\ 
  Stacking Model 2 (average, three) & 0.78 & 0.63 & 0.24 & 0.18 \\ 
  Stacking Model 3 (GB, all) & 0.80 & 0.63 & 0.30 & 0.16 \\ 
  Stacking Model 4 (LR, all) & 0.81 & 0.65 & 0.26 & 0.17 \\ 
   \hline
\end{tabular}
\caption{Model performance on the test dataset.}\label{eval}
\end{table}

Since all models were tuned on the AUC, the comparison on behalf of this metric is most informative. The two Ensemble models, namely the Random Forest and the Gradient Boosting model, score highly on the AUC metric with a value of $0.78$. In terms of AUC they even outperform most level 0 generalizers, especially the Decision Tree, the Logistic Regression and the Neural Network. It can be concluded that they rank a randomly chosen correct prediction comparatively comparatively higher than a randomly chosen false prediction in comparison with the level 0 generalizers. Notably, the Gradient Boosting level 0 generalizer and the Random Forest level 0 generalizer perform a bit different when compared to their counterparts. This may origin from the decreased training dataset size due to the partitioning that was applied for the level 0 generalizers. 

The four Stacking models were built on top of the level 0 generalizers. Figure \ref{corrgram} shows the correlations of the predictions of the level 0 generalizers on the training data. It can be seen that the predictions of the level 0 generalizers are not correlated too strongly ($<|0.8|$). This implies diversity, which already indicates that combining them may increase predictive performance.

\begin{figure}[htp] \centering{
\includegraphics[trim={1cm 3.5cm 1cm 0.6cm}, scale=0.8]{graphs/corrgram_version2.pdf}}
\caption{Correlation plot of training dataset predictions of level 0 generalizers.}\label{corrgram}
\end{figure}  

Indeed, Table \ref{eval} reveals that the four Stacking models clearly outperform their level 0 generalizers in terms of AUC. Stacking model 1 and Stacking model 2 that are based on averaging predictions have similar high AUC performance than the Random Forest and the Gradient Boosting model. Stacking model 3 and Stacking model 4 that are based on combining level 0 predictions by Gradient Boosting and Logistic Regression, respectively, even show a better AUC performance than the Random Forest and the Gradient Boosting model. 

With regards to the Accuracy, the Logistic Regression and the Neural Network perform about equally well as the two Ensemble models. The Decision Tree is however outperformed by the Random Forest and the Gradient Boosting model. Interestingly, all four Stacking models show relatively bad Accuracy values. Regarding the relation of Accuracy and AUC, it can be concluded that the Stacking and Ensemble models do not necessarily perform better on the cut-off threshold of $0.5$, which is implied by the Accuracy metric. However, generalizing over all possible cut-off threshold they clearly outperform most level 0 generalizers as can be seen by the corresponding AUC values. Furthermore, this indicates that the standard cut-off threshold for probabilistic predictions of $0.5$ may not be optimal for those models.

The Logarithmic Loss metric gives even more information on the particular strengths and weaknesses of the models. Contrasting the level 0 generalizers with the Random Forest and the Gradient Boosting model, the latter show a slightly smaller Logarithmic Loss. The four Stacking models however, reveal a much better performance on behalf of the Logarithmic Loss metric. It can be concluded, that the Ensemble models and even more the Stacking models do not place as high confidences on their incorrect predictions. 

Regarding the Brier Score, differences are not that present. In tendency, the Stacking and Ensemble Models perform slightly better on that metric than the level 0 generalizers. One time more, this shows that in binary classification a good model is not only defined by its errors but rather by which observations it is able to classify correctly.

With regards to all calculated metrics, Stacking models are be able to capture the structure in the data most effective. Already a simple averaging approach, as applied for Stacking models 1 and 2, leads to increased performance. Interestingly, reducing the set of level 0 generalizers, like for Stacking model 2, does not add more value. Nevertheless, it shall be noted that such restriction of input predictions could still decrease computational costs. Applying a more sophisticated level 1 combiner like Gradient Boosting or Logistic Regression, does increase performance slightly as can be seen by Stacking models 3 and 4.

[evtl.noch einen Satz dazu, dass aus den level 0 generalizern einzig das NeuralNetwork annähernd vergleichbar hohe performance brachte]








\section{Conclusion}
This paper aimed to discuss and evaluate Stacking and Ensemble models in a financial application of credit risk assessment. Focus was set on introducing the concepts of Bagging, Boosting and Stacked Generalization as well as the corresponding Random Forest and Gradient Boosting models. It was explained how Bagging and Boosting increase performance by decreasing generalization variance of base learners and by decreasing generalization bias of base learners, respectively. Moreover, the Stacked Generalization or Stacking model seeks for the optimal way to combine the specific strengths of level 0 generalizers in order to increase predictive performance even more.

A broad set of different Stacking and Ensemble models as well as standard machine learning models was applied and evaluated to a classification problem of credit risk assessment. The Random Forest model and Gradient Boosting model outperformed standard machine learning models on behalf of the calculated metrics. Furthermore, all four Stacked Generalization approaches were able to establish a higher performance on AUC, Logarithmic Loss and Brier Score. With regards to Accuracy metric, they performed worse, suggesting the metric-implied cut-off threshold of $0.5$ being sub-optimal. Even a simple combiner algorithm like averaging could increase performance, while more complex combiner algorithms performed better. Restricting the subset of input predictions seems to be ineffective with regards to performance issues.

In this study's context of credit risk assessment, Stacking and Ensemble models could show their comparative strengths when it comes to prediction of a binary outcome. Further research could however evaluate their performance for regression problems in a similar financial context. Beyond that, Stacking and Ensemble models need much computational resources due to their complexity. In the context of \textit{scalability}, current research tests ideas that restrict computational costs, e.g. by adaptive parallelization \citep{li2014communication} or by improving certain methods that are like stochastic gradient descent \citep{bottou2012stochastic}. A further problem of such models is absence of proven statistical properties like unbiasedness, consistency or asymptotic theory for construction of confidence intervals. In order to use machine learning models for research purposes, such properties are however necessary. Only recently, the field of \textit{machine learning in economics} emerged, where the construction of such properties is aimed at (see \cite{athey2017impact} ,\citeauthor{wager2018est} \hyperlink{wager2018est}{(\color{DarkBlue}{in press})}).  







%----------------------------------------------------------------------------------------
%	APPENDIX
%----------------------------------------------------------------------------------------
\newpage
\appendix
\section{Appendix - Summary Tables}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & Mean & Std. Dev. & Median & Minimum & Maximum \\ 
  \hline
Duration & 20.90 & 12.06 & 18.00 & 4.00 & 72.00 \\ 
  Amount & 3271.26 & 2822.74 & 2319.50 & 250.00 & 18424.00 \\ 
  Installment Rate & 2.97 & 1.12 & 3.00 & 1.00 & 4.00 \\ 
  Residence Duration & 2.85 & 1.10 & 3.00 & 1.00 & 4.00 \\ 
  Age & 35.55 & 11.38 & 33.00 & 19.00 & 75.00 \\ 
  Number of Credits & 1.41 & 0.58 & 1.00 & 1.00 & 4.00 \\ 
  Number of Liable People & 1.16 & 0.36 & 1.00 & 1.00 & 2.00 \\ 
   \hline
\end{tabular}
\caption{Summary statistics for numerical features in the German Credit Dataset.}\label{summary_numeric}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{rllll}
  \hline
Feature & Category & Count & Fraction \\ 
  \hline
Customer Classification & good & 700 & 70\% \\ 
(Outcome Feature) &bad & 300 & 30\% \\
\hline
Account Status & x < 0 DM & 274 & 27.4\% \\ 
 & 0 DM < x < 200 DM & 269 & 26.9\% \\ 
 & x >= 200 DM & 63 & 6.3\% \\ 
& no account & 394 & 39.4\% \\ 
\hline
Credit History & no credits taken/all paid back duly & 40 & 4\% \\ 
& all credits at this bank paid back duly & 49 & 4.9\% \\ 
 & existing credits paid back duly till now & 530 & 53\% \\ 
 & delay in paying off in the past & 88 & 8.8\% \\ 
 & critical account & 293 & 29.3\% \\ 
\hline
 Purpose & car (new) & 234 & 23.4\% \\ 
 & car(used & 103 & 10.3\% \\ 
&furniture/equipment & 12 & 1.2\% \\ 
&radio/television & 181 & 18.1\% \\ 
& domestic appliances & 280 & 28\% \\ 
 &repairs & 12 & 1.2\% \\ 
 &education & 22 & 2.2\% \\ 
 &vacation & 50 & 5\% \\ 
 &retraining & 9 & 0.9\% \\ 
 &business & 97 & 9.7\% \\ 
\hline
 Savings & x < 100 DM & 603 & 60.3\% \\ 
&100 <= x <  500 DM & 103 & 10.3\% \\ 
&500 <= x < 1000 DM & 63 & 6.3\% \\ 
&x >= 1000 DM & 48 & 4.8\% \\ 
&unknown/no savings & 183 & 18.3\% \\ 
\hline\\
(continued on next page) & & & 
\end{tabular}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{rllll}
 (continued)& & &\\ \\
\hline
Feature & Category & Count & Fraction \\ 
  \hline
 Employment Duration & unemployed & 62 & 6.2\% \\ 
&x < 1 year & 172 & 17.2\% \\ 
&1  <= x < 4 years & 339 & 33.9\% \\ 
&4  <= x < 7 years & 174 & 17.4\% \\ 
&x >= 7 years & 253 & 25.3\% \\ 
\hline
Status and Sex & male: divorced/separated & 50 & 5\% \\ 
&female: divorced/separated/married & 310 & 31\% \\ 
&male: single & 548 & 54.8\% \\ 
&male: married/widowed & 92 & 9.2\% \\ 
\hline
 Other Debtors & none & 907 & 90.7\% \\ 
&co-applicant & 41 & 4.1\% \\ 
&guarantor & 52 & 5.2\% \\ 
\hline
  Property & real estate & 282 & 28.2\% \\ 
&savings agreement/life insurance & 232 & 23.2\% \\ 
&car or other& 332 & 33.2\% \\ 
&unknown/no property & 154 & 15.4\% \\ 
\hline
 Other Installment Plans & bank & 139 & 13.9\% \\ 
&stores & 47 & 4.7\% \\ 
&none & 814 & 81.4\% \\ 
\hline
Housing & rent & 179 & 17.9\% \\ 
&own & 713 & 71.3\% \\ 
&for free & 108 & 10.8\% \\ 
\hline
 Job & unemployed/ unskilled  - non-resident & 22 & 2.2\% \\ 
 & unskilled - resident & 200 & 20\% \\ 
 &skilled employee / official & 630 & 63\% \\ 
 &management/self-employed/officer & 148 & 14.8\% \\ 
 \hline
Telephone & none & 596 & 59.6\% \\ 
&yes & 404 & 40.4\% \\ 
 \hline
Foreign Worker & yes & 963 & 96.3\% \\ 
&no & 37 & 3.7\% \\ 
    \hline
\end{tabular}
\caption{Summary statistics for categorical features in the German Credit Dataset.}\label{summary_categorical}
\end{table}



\newpage
\section{Appendix-part2}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
\clearpage
\bibliographystyle{apa}
\bibliography{library}
\end{document}
