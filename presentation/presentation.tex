\documentclass{beamer}

\usepackage{graphicx}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{eso-pic}
\usepackage{mathrsfs}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{booktabs}
% \usepackage{bbm}
\usepackage{cooltooltips}
\usepackage{colordef}
\usepackage{beamerdefs}
\usepackage{lvblisting}
\usepackage{setspace} % to be able to temporarily change "Zeilenabstand"
\usepackage{sansmathaccent} % to be able to use math without error
\pdfmapfile{+sansmathaccent.map}

\usepackage[square,sort]{natbib}

\pgfdeclareimage[height=2cm]{logobig}{hulogo}
\pgfdeclareimage[height=0.7cm]{logosmall}{Figures/ensemble.png}

\usepackage{listings} % package to insert code

\renewcommand{\titlescale}{1.0}
\renewcommand{\titlescale}{1.0}
\renewcommand{\leftcol}{0.6}

\title[Stacking Algorithm for Ensemble Modelling]{Stacking Algorithm for Ensemble Modelling}
\authora{Frederik Schreck}
\authorb{}
\authorc{}

\def\linka{}
\def\linkb{}
\def\linkc{}

\institute{Course: Numerical Introductory Course \\
Lecturer: Prof. Dr. Brenda López Cabrera
Humboldt-University Berlin \\}

\hypersetup{pdfpagemode=FullScreen}

\begin{document}

% 0-1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame[plain]{
\titlepage
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
\frametitle{Motivation - The wisdom of the crowd}
\begin{itemize}
\item The aggregation of individual guesses	 in groups is often superior to individual guesses - even to experts
\item BUT: Only fulfilled under certain criteria \citep{Surowiecki:2005:WC:1095645}
	\begin{itemize}
	\item Variation of guesses
	\item Independence of guesses
	\item Decentralization 
	\item Algorithm
	\end{itemize}
\end{itemize}
}

%TEXT: 
%Start with this story: In 1906, Francis Galton's went to a county fair, and attended an interesting contest. People were asked to guess the %weight of an ox. About 800 visitors of the fair gave a guess, but no one in the end got the exact result: 1.198 pounds. %However, when aggregating the individual guesses, the average was really close: 1,197 pounds. 

% The concept behind this is known as "wisdom of the cloud"

%	Independence of guesses = people decide independent from each other. Or only small correlation of individual guesses. I will come back later to this point.
% 	Decentralization = 	People are able to specialize and draw their knowledge (?); 
%	Algorithm = results clearly depend on whether mean or median or s.th. else

% Exactly, this is the motica

%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Outline}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
\frametitle{Outline}

\begin{enumerate}
\item Motivation \quad \checkmark
\item Background: Decision Tree
\item Ensemble Learning
\item Stacking Algorithms and Ensemble Modelling
\begin{enumerate}
\item Bagging and Random Forest
\item Boosting and Gradient Boosting
\item Stacked Generalization
\end{enumerate}
\item Potentials and Problems of Ensemble Learning
\item Sources
\end{enumerate}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Decision Tree}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
\frametitle{Decision Tree}
\begin{itemize}
\item Idea: Use a set of splitting rules to recursively partition the dataset
\item Classification trees:
\begin{itemize}
\item Minimize impurity within nodes
\end{itemize}
\item Regression trees:
\begin{itemize}
\item Minimize variance of the response variable within nodes
\end{itemize}
\end{itemize}
}
% Draw two pictures for classification tree: one twodimensional graph (x-axis = age, y-axis = wage; Dependente var = risky customer (YES/NO); draw some dots into graph and separate them by lines while explaining the tree. Second plot is the tree structure.--> Explain how tree is build. And then explain how tree is used for prediction of a new datapoint.)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Decision Tree}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Since I evaluate a classification task in my paper, I will here focus on classification tree (and later stacking models for classification tasks.)
\frame{
\frametitle{Decision Tree for classification}
\begin{itemize}
\item Choice of splitting rule: maximizing information gain (IG) by decreasing node impurity (I) 
\begin{flalign}
IG_n = I(n) - p_{n_1} \times I(n_1) - p_{n_2} \times I(n_2),
\end{flalign}
for node $n$ with branching nodes $n_1$ and $n_2$, and $p_{n_i}$ as the fraction of cases in branching node $n_i$, $i\in\{1,2\}$\\
\end{itemize}
}

\frame{
\frametitle{Decision Tree for classification}
\begin{itemize}
\item How to measure impurity? Choices of splitting criteria:
\begin{flalign}
&\text{Entropy: } I(n) = -\sum_{j}^{J} p(c_j|n) * \log_{2}(p(c_j|n))\\
% some sort of average rate in which information is lost in node n.
&\text{Gini impurity: } I(n) = 1 - \sum_{j}^{J} p(c_j|n)^2\\
%Expected error rate at node N if the category label is selected randomly from the class distribution present at N. MEANS: Prob of miscklassifyoing a randomly chosen element in node N.
&\text{Misclassification impurity: } I(n) = 1 - \max_{{j}}	p(c_j),
%Minimum probability that a training pattern will be misclassified at node n
\end{flalign}
for node $n$ and classes $c_j, j \in \{1,2,...,J\}$
\end{itemize}
}

\frame{
\frametitle{Decision Tree for classification}
\begin{itemize}
\item Choice of stopping rule:\\
A fully grown tree has pure leaf nodes and may overfit the data.
However, a small tree may not capture all relevant structure of the data
\begin{itemize}
\item Pre-pruning
\item Post-pruning
\end{itemize}
\end{itemize}
}
%prepruning means to not grow tree fully but stop earlier-->Choice of stopping criteria (e.g. maximal depth)
%postpruning means to first fully grow the tree, and then cut the branches that are less important w.r.t. e.g. the decrease of impurity. For that we use new data.
%--> Decision trees are prone to overfitting. We can however correct for this by the methods I will introduce you to later.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ensemble Learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
\frametitle{Ensemble Learning - Terminology}
Machine Learning
\begin{itemize}
\item Part of computer science that uses statistical techniques to train models on data
\item Typically used for prediction purposes
\end{itemize}
\vspace{0.5cm}
Stacking and Ensemble Learning
\begin{itemize}
\item Idea is to combine hypotheses of multiple learning algorithms (base learners)
% explain word "hypothesis". So we combine the base learners' hypotheses in order to get a new hypothesis. Finite set of base learners used. 
\item Goal is to obtain a better predictive performance than with each of the single algorithms alone
\item Mainly used in supervised learning
% There have been some applications in unsupervised models, like for example "consensus clustering" but in general ensembling focuses on supervised learning.
\item Very flexible method
% The ensembling hypothesis, however, is not necessarily contained within the hypothesis space of the models from which it is built.Ensembles can be shown to have more flexibility in the functions they can represent.

% Notes: Models - Learners, predictions - hypotheses,...
\end{itemize}
}


\frame{
\frametitle{Ensemble Learning}
Which models to combine?
\begin{itemize}
\item Effective ensembling builds on diverse and little correlated models \citep{kuncheva2003measures}
\item Typically best to use strong base learners
% Research has been shown that ensembling is most effective when base learners already have good predictions.
\item Choice of algorithm decisive
% SO, we see the same criteria than I mentioned in the motivation of this presentation! Also clearly the choice of algorithm is important. In the next sections, I will introduce different stacking models in ensemble modelling. 
\end{itemize}
\vspace{0.5cm}
Compare with the criteria mentioned in the Motivation!
}

\frame{
\frametitle{Ensemble Learning}
Which models to combine?
\begin{figure}[htb]
	\begin{center}
	\includegraphics[scale=0.38, trim={0.5cm 1.3cm 0 0}]{Figures/bias_variance_tradeoff.png}
	\caption{The bias-variance-trade-off.}
	\end{center}
\end{figure}
\begin{itemize}
\item Combining complex classifiers may reduce variance
% e.g. averaging over many estimates gives more robust estimation
\item Combining simple classifiers may reduce bias
% combining simple rules gives eventually a complex rule
\end{itemize}
}


%\frame{
%\frametitle{Bias-Variance-Trade-Off for Decision Trees}
%\begin{figure}[htb]
%	\begin{center}
%	\includegraphics[scale=0.26, trim={0cm 1cm 0 0.5cm}]{Figures/bvtradeoff_tree.png}
%	\caption{The bias-variance-trade-off for decision trees.}
%	\end{center}
%\end{figure}
%\begin{itemize}
%\item So how do we obtain a good model?
%\end{itemize}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bagging and Random Forest}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
\frametitle{Bagging (= Bootstrap Aggregating)}
\begin{itemize}
\item Proposed by Leo Breiman \citep{breiman1996bagging}
\item Meta-algorithm, designed to
\begin{itemize}
\item improve accuracy of base algorithms
\item reduce MSE by reducing variance % bias is not changed due to bagging
\item avoid overfitting problems
\item obtain smoother prediction boundaries
% possibly draw a two dimensional coordinate system with two features as axes and two colored dots (group 1 as a circle). Then draw lines from different (small) trees and show that by drawing many lines, the prediction boundary becomes smooth.
\end{itemize} 
\item Can be applied to all kinds of base learners
\item However best to use unstable methods that tend to have high variance, like trees
\end{itemize}
}

\frame{
\frametitle{Bagging algorithm}
Suppose we have training data $\{(x_1,y_1),...,(x_N,y_N)\}$
\small
\rule{10cm}{0.05cm}\\
\begin{enumerate}
\item[1] for base learner $m$ in $\{1,2,...,M\}$
\item[2] \hspace{1cm} uniformly draw sample $D_m$ from dataset $D$ (with repl.)
\item[3] \hspace{1cm} build model $T_m$ on dataset $D_m$ to obtain hypothesis $h_m(x)$
\item[4] combine hypotheses 
\end{enumerate}
\rule{10cm}{0.05cm}\\
\begin{itemize}
\item Combining by averaging in regression problems
\item Combining by majority vote in classification problems
\end{itemize}
}

\frame{
\frametitle{Random Forest}
\begin{itemize}
\item Also proposed by Leo Breiman \citep{breiman2001random}
\item Random forests combine bagging with random subspace approach \citep{ho1998random}
\item Random subspace approach randomly samples features from set of all features for each learner (with replacement)
\begin{itemize}
\item Reduces the correlation between estimators
\item Thus decreases variance in the ensemble learner
\end{itemize}
\item Random feature sampling happens at tree level or at split level
\item Random Forest only possible with tree-based base learners
\end{itemize}
}

\frame{
\frametitle{Random Forest algorithm for classification}
Suppose we have training data $\{(x_1,y_1),...,(x_N,y_N)\}$
\small
\rule{10cm}{0.05cm}\\
\begin{enumerate}
\item[1]for base learner $m$ in $\{1,2,...,M\}$
\item[2]\hspace{1cm}uniformly draw sample $k_m$ of size $L$ from features $\{1,2,...,K\}$ \\\hspace{1cm}(with repl.)
\item[3]\hspace{1cm}uniformly draw sample $D_m$ from dataset $D$ (with repl.)
\item[4]\hspace{1cm}build model $T_{m}$ on dataset $D_m$ using feature set $k_m$ 
\item[5]$\hat{C}_{rf}^{L,N}(x) = \text{majority vote}\{\hat{T}_{m}\}_{1}^{M}$
\end{enumerate}
\rule{10cm}{0.05cm}\\
}
% which are the tunable parameters: number of trees (the more the better, but computational costs increase), number of features for each tree, bootstrap sample sizes, number of branches for trees OR other sort of pruning...


\frame{
\frametitle{Random Forest}
Random Forest vs. single Tree\\
\begin{table}
\centering
\begin{tabular}{p{.48\textwidth}|p{0.45\textwidth}}
Random Forest & Single Tree \\
\hline
\begin{itemize}
\item[$-$] higher computational costs %lot of storage memory and time
\item[$-$] blackbox %but importanceplots
\item[$-$] many parameter choices to make
\item[$+$] easy to tune parameters
\item[$+$] smaller prediction variance % so better performance
\item[$+$] scalability

\end{itemize}
&
\begin{itemize}
\item[$+$] computationally simple
\item[$+$] insights into decision rules
\item[$+$] easy to tune parameters
\item[$-$] tends to overfit and have high variance
\end{itemize}
\end{tabular}
\end{table}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Boosting and Gradient Boosting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
\frametitle{Boosting}
\begin{itemize}
\item Method proposed by Freund \& Shapire \citep{freund1996experiments}
\item Original idea only applies to classification problems
\item Idea: Simple learners are easier to find. Combining many simple learners can produce a powerful learner
\item The ensemble first considers only one base learner. Then we iteratively enlarge it by another base learner that aims to correct the error of the current ensemble
\end{itemize}
}
% Draw a two dimensional graph again on the board. And separate datapoints by line of first base learner. Then draw the misclassified dots bigger and the other smaller. Then separate by a line of the second base learner.... In the end combine all lines.

\frame{
\frametitle{The Adaboost algorithm}
Suppose we have training data $\{(x_1,y_1),...,(x_N,y_N)\}$,\\
initialize weightings $d_i^{(1)}=\frac{1}{N}, \forall i \in \{1,...,N\}$
\scriptsize
\rule{10cm}{0.05cm}\\
\begin{enumerate}
\item[1]for base learner $m$ in $\{1,2,...,M\}$
\item[2]\hspace{1cm}train base learner according to weighted data $d^{(m)}$\\
\hspace{1cm}and obtain hypothesis $h_m:\textbf{x}\mapsto\{-1,+1\}$
\item[3]\hspace{1cm}calculate weighted classifier error\\
\hspace{1cm}$\epsilon_m=\sum_{i=1}^{N}d_i^{m}I(y_i\neq h_t(x_i))$
\item[4]\hspace{1cm}calculate hypothesis weighting $\beta_m=\frac{1}{2}\log(\frac{1-\epsilon_m}{\epsilon_m})$
\item[5]\hspace{1cm}update data weighting, e.g. by\\
\hspace{1cm}$h_m(x_i)=y_i:d_i^{m+1}=d_i^{m}\exp(-\beta_m)$\\
\hspace{1cm}$h_m(x_i)\neq y_i:d_i^{m+1}=d_i^{m}\exp(\beta_m)$ 
\item[6]$\hat{y}(x)=H_{final}(x)=\frac{1}{M}\sum_{1}^{M}\beta_mh_m(x)$
\end{enumerate}
\rule{10cm}{0.05cm}
}
% So the sample distribution/data weightings are different for different base learners.
% The classifier weights \beta_m depend on which cases are classified correctly and which are classified incorrectly.(this is such a lying s-shaped function-->draw on board)
% The updated weightings of each case	 depend on how many base classifiers misclassified this case.

\frame{
\frametitle{Gradient Boosting}
\begin{itemize}
\item Developed by Friedman \citep{friedman2001greedy}
\item Extended boosting to regression and other problem types
\item Shortcomings of current ensemble is identified by gradients instead of weightings of data
\item In each stage $m\in \{1,2,...,M\}$, a new learner improves the current ensemble $H_{m-1}$ and is fitted to $(x_i, y_i - H_{m-1}(x_i)), \forall i \in\{1,2,...,N\}$
\end{itemize}
}
%Def. Gradient descent: Gradient descent is a first?order optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point

\frame{
\frametitle{Stochastic Gradient Boosting}
\begin{itemize}
\item Advancement of Gradient Boosting, again by Friedman \citep{friedman2002stochastic}
\item Utilizes ideas from Bagging: 
\begin{itemize}
\item Using trees as base learners
\item Fit trees to negative gradient of random sample of dataset
\end{itemize}
\item Less prune to overfitting
\end{itemize}
}
% parameters to tune, mainly focus on preventing overfitting. E.g.Ensemble size, tree parameters, shrinkage term, minimum observations per node in tree,...

\frame{
\frametitle{Gradient Boosting}
Random Forest vs. single Tree vs. Gradient Boosting\\
\begin{table}
\scriptsize
\centering
\begin{tabular}{p{.28\textwidth}|p{0.28\textwidth}|p{0.33\textwidth}}
Random Forest & Single Tree & Gradient Boosting\\
\hline
\begin{itemize}
\item[$-$] higher computational costs %lot of storage memory and time
\item[$-$] blackbox %but importanceplots
\item[$+$] easy to tune parameters
\item[$+$] smaller prediction variance % so better performance
\item[$+$] scalability
\item[$-$] many parameter choices to make
\end{itemize}
&
\begin{itemize}
\item[$+$] computationally simple
\item[$+$] insights into decision rules
\item[$+$] easy to tune parameters
\item[$-$] tends to overfit and have high variance
\end{itemize}
&
\begin{itemize}
\item[$+$] relatively fast to train
\item[$+$] insights by feature importance and partial dependence plots
\item[$+$] one of the best of-the-shelf methods
\item[$-$] tends to overfit
\item[$-$] parallelization difficult
\item[$-$] many tunable parameters
\end{itemize}
\end{tabular}
\end{table}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Stacked Generalization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
\frametitle{Stacked Generalization}
\begin{itemize}
\item Heterogeneous ensemble model % since different prediction methods are used
\item Second stage model: Combines predictions of different classifiers % So first we run those, then we seek to combine them by some technique
\item Multistep-Approach: 
\begin{enumerate}
\item Compare predictive accuracy of individual classifiers
\item Select the best individual classifier
\item Iteratively add one more classifier and combine the predictions, e.g. by averaging
\item Stop when predictive accuracy of the ensemble does not increase anymore
% base classifiers can be selected multiple times here (equals a weighting scheme)
\end{enumerate}
\end{itemize}
}

\frame{
\frametitle{The Multistep-Approach}
An example:
\begin{table}
\tiny
\centering
\begin{tabular}{|p{.2\textwidth}|p{.1\textwidth}|p{0.1\textwidth}|p{0.1\textwidth}|p{0.1\textwidth}|}
\hline
Iteration & y & $\hat{y}_1$ & $\hat{y}_2$ & $\hat{y}_3$\\
\hline
 	1 & 0 & 0.53 & 0.17 & 0.07 \\ \cline{2-5}
	  & 0 & 0.62 & 0.61 & 0.95 \\ \cline{2-5}
	  & 1 & 0.76 & 0.41 & 0.31 \\ \cline{2-5}
\hline \hline
	Brier Score: & & \textbf{0,24} & 0.25 & 0.46 \\ \cline{2-5}
\hline
\end{tabular}
\end{table}
\begin{table}
\tiny
\centering
\begin{tabular}{|p{.2\textwidth}|p{.1\textwidth}|p{0.1\textwidth}|p{0.1\textwidth}|p{0.1\textwidth}|}
\hline
Iteration & y & $\overline{(\hat{y}_1,\hat{y}_1)}$ & $\overline{(\hat{y}_1,\hat{y}_2)}$ & $\overline{(\hat{y}_1,\hat{y}_3)}$\\
\hline
 	2 & 0 & 0.53 & 0.35 & 0.30 \\ \cline{2-5}
	  & 0 & 0.62 & 0.62 & 0.79 \\ \cline{2-5}
	  & 1 & 0.76 & 0.59 & 0.54 \\ \cline{2-5}
\hline \hline
	Brier Score: & & 0.24 & \textbf{0.22} & 0.31 \\ \cline{2-5}
\hline
\end{tabular}
\end{table}
\begin{table}
\tiny
\centering
\begin{tabular}{|p{.2\textwidth}|p{.1\textwidth}|p{0.1\textwidth}|p{0.1\textwidth}|p{0.1\textwidth}|}
\hline
Iteration & y & $\overline{(\hat{y}_1,\hat{y}_2,\hat{y}_1)}$ & $\overline{(\hat{y}_1,\hat{y}_2,\hat{y}_2)}$ & $\overline{(\hat{y}_1,\hat{y}_2,\hat{y}_3)}$\\
\hline
 	2 & 0 & 0.41 & 0.29 & 0.26 \\ \cline{2-5}
	  & 0 & 0.62 & 0.61 & 0.73 \\ \cline{2-5}
	  & 1 & 0.64 & 0.53 & 0.49 \\ \cline{2-5}
\hline \hline
	Brier Score: & & \textbf{0.225} & 0.228 & 0.280 \\ \cline{2-5}
\hline
\end{tabular}
\end{table}
}


\frame{
\frametitle{Stacked Generalization}
\begin{itemize}
\item Most important: Diversity of first-stage models! % Idea behind that is, that the different models are good in explaining part of the structure in the data. Combining them will then be able to explain a larger part of the structure in the data.
\item Diversity can be enhanced by using
\begin{itemize}
\item different algorithms
\item different parameter settings
\item different feature subsets
\item different training sets
\end{itemize}
\item Overfitting problem. Can be evaded by % due to the fact that the first stage learners all predict the same, we have typically high multikollinearity
\begin{itemize}
\item using cross-validation in second-stage model training
\item using regularization
\item using a combiner algorithm that is not sensible to multicollinearity, like Regularized Logistic Regression, Random Forest or hill-climbing methods.
\end{itemize}
\end{itemize}
}


\frame{
\frametitle{Stacked Generalization}
\begin{table}
\small
\centering
\begin{tabular}{p{.45\textwidth}|p{0.45\textwidth}}
Advantages & Disadvantages\\
\hline
\begin{itemize}
\item[$+$] often 
 high predictive accuracy and robustness (most successful approach in data science competitions on \textit{kaggle})
\item[$+$] scalable, especially parallelizable
\end{itemize}
&
\begin{itemize}
\item[$-$] complex to implement
\item[$-$] high amount of computational resources needed	
\item[$-$] different (!) first-stage models must be built
\end{itemize}
\end{tabular}
\end{table}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Potentials and Problems of Ensemble Learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{
\frametitle{Potentials of Ensemble Learning}
\begin{figure}[htb]
	\begin{center}
	\includegraphics[scale=0.26, trim={0cm 1cm 0 0.5cm}]{Figures/bvtradeoff_boosting_bagging.png}
	\caption{How Gradient Boosting and Random Forest improve performance.}
	\end{center}
\end{figure}
}

\frame{
\frametitle{Potentials and Problems of Ensemble Learning}
\begin{table}
\centering
\begin{tabular}{p{.45\textwidth}|p{0.45\textwidth}}
Potentials & Problems\\
\hline
\begin{itemize}
\item[$+$] currently best predictive methods available
\item[$+$] ensembling can decrease variance and bias
\item[$+$] often scalable
\end{itemize}
&
\begin{itemize}
\item[$-$] needs high computational resources 
\item[$-$] blackbox problems
\item[$-$] many parameters to tune
\item[$-$] lack of proven statistical properties 
% actually there is much research on that, especially in emerging field "ML in Economics". For example I found a paper (by Athey) that could prove some consistency of a certain kind of causal random forest.
\end{itemize}
\end{tabular}
\end{table}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sources}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame[allowframebreaks]{
\frametitle{References}
\begingroup
\renewcommand{\section}[2]{}%
\bibliographystyle{apa}
\bibliography{library}
\endgroup
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
