% TeX-Engine = pdflatex
% !TeX program = lualatex
\documentclass[12pt]{article}


% Packages
% ========
\usepackage{mathtools, amssymb}


%\usepackage[math-style=TeX,bold-style=upright]{unicode-math}
%\setmathfont[Ligatures=TeX]{XITS Math}

%\usepackage{fontspec}
%\setmainfont[Ligatures=TeX]{Lato}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

%\usepackage{polyglossia}
%\setdefaultlanguage{english}
\usepackage[english]{babel}
\usepackage{lipsum}


\usepackage[a4paper, top=1in, bottom=1.25in, left=1in, right=1in]{geometry}
\usepackage[onehalfspacing]{setspace}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[L]{}
\fancyhead[C]{}
\fancyhead[R]{\nouppercase{\leftmark}}
\fancyfoot[L]{}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{}

\usepackage[labelfont=bf, font=small,justification=justified, singlelinecheck=false]{caption}
\usepackage{booktabs}
\usepackage{multirow}

\usepackage{graphicx}
%\usepackage{float}
%\usepackage{subfig}

\usepackage{tabularx,ragged2e}
%\newcolumntype{C}{>{\centering\arraybackslash}X}

\usepackage{url} % nicer url in references

\usepackage{flafter}

\usepackage[svgnames,table,xcdraw]{xcolor}

\usepackage{listings}
\lstset{    
    language        =R,
    backgroundcolor =\color{GhostWhite},
    basicstyle      =\ttfamily,
    breaklines      =true,
    commentstyle    =\color{Gray},
    keepspaces      =false,
    keywordstyle    =\color{black},
    deletekeywords  ={data,_},
    morekeywords    ={},
    numbers         =left,
    numbersep       =8pt,
    numberstyle     =\tiny,
    rulecolor       =\color{black},
    showstringspaces=false,
    showtabs        =false,
    showstringspaces=false,
    stringstyle     =\color{black},
    tabsize         =2,
    frame           =single
}

\usepackage[title]{appendix}
\usepackage{enumitem}

\usepackage[most]{tcolorbox}
%\newtcolorbox{mthm}{breakable, enhanced, sharp corners, colframe=black, colback=GhostWhite, coltitle=black}

\usepackage{tikz}
\usetikzlibrary{positioning,shadows}

%\usepackage{filecontents}

%\usepackage[backend=biber, minbibnames=3, bibstyle=authoryear, citestyle=authoryear, natbib, sorting=nyt]{biblatex}
%\DefineBibliographyStrings{english}{andothers = {et\addabbrvspace al\adddot}}
%\addbibresource{report.bib}
\usepackage[round]{natbib}

%\usepackage[unicode,colorlinks,filecolor=DarkGreen,citecolor=DarkRed,linkcolor=black,urlcolor=DarkBlue]{hyperref}
\usepackage[colorlinks, filecolor=magenta, citecolor=DarkBlue, linkcolor=black, urlcolor=blue]{hyperref}

\usepackage{wrapfig}

%\usepackage{icomma}

%\newcommand{\Quantlet}[2]{\includegraphics[scale = 0.05]{q.pdf}\,\href{#2}{#1}}

\title{Stacking algorithm and ensemble modeling}
\author{Frederik Schreck}
\date{Berlin, \today}


\renewcommand{\familydefault}{\rmdefault} % is computer modern sans serif

\begin{document}


%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{titlepage} % Suppresses displaying the page number on the title page and the subsequent page counts as page 1
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for horizontal lines, change thickness here
	

	%------------------------------------------------
	%	Headings
	%------------------------------------------------

	\begin{minipage}{0.77\textwidth}
		\begin{flushleft}
			\large
			\textsc{Humboldt University of Berlin\\
			Course: Numerical Introductory Seminar\\
			Supervisor: Prof. Dr. Brenda LÃ³pez Cabrera\\}
		\end{flushleft}
	\end{minipage}
		~
	\begin{minipage}{0.2\textwidth}
		\begin{flushleft}
		\includegraphics[width=0.73\textwidth]{graphs/HU_logo.png}\\
		\end{flushleft}
	\end{minipage}
	
	\vspace{2cm}

	%------------------------------------------------
	%	Title
	%------------------------------------------------
	\center
	\HRule\\[0.4cm]
	
	{\LARGE\bfseries Stacking Algorithm for Ensemble Modelling}\\[0.05cm] % Title of your document
		\HRule\\[1.5cm]
	
	%------------------------------------------------
	%	Author(s)
	%------------------------------------------------
			
			\textit{Submitted by}\\	
			\vspace{0.5cm}
			

	\begin{minipage}{0.5\textwidth}
		\begin{center}
			\large

			\textsc{Frederik Schreck \\
			Statistics M.Sc.\\
			Humboldt University\\
			580567} % Your name
		\end{center}

	\end{minipage}
	

	\vfill\vfill\vfill\vfill % Position the date 3/4 down the remaining page
	
	{\large\today} % Date, change the \today to a set date if you want to be precise
	
	\vfill % Push the date up 1/4 of the remaining page
	
\end{titlepage}

%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------

\tableofcontents
\thispagestyle{empty}
\clearpage
\setcounter{page}{1}
\newpage

\listoffigures
\listoftables
\thispagestyle{empty}
\clearpage
\setcounter{page}{1}


%%
%%
%%
%%
\section{Motivation (with simple example)[1page]}\label{Intro}
"[...] when our imperfect judgments are aggregated in the right way, our collective intelligence is often excellent."\citep[Foreword p.XIV]{surowiecki2005wisdom}\\

In accordance with the title of his book, Surowiecki refers to what he calls the \textit{wisdom of crowds}-effect \cite{surowiecki2005wisdom}. By that, he describes the social phenomen, that - under certain criteria - the aggregates of individual judgements can be superior to each of the individual judgements. While this effect holds true for social science, it also applies to the world of statistics and machine learning. In the field of stacking and ensemble modelling, research has shown different ways in which aggregation of predictive models can deliver a more powerful model. Regarding the diversity of existing approaches, this paper aims to apply and discuss the most established ones in a financial application setting. In depth, the concept of Bagging and the Random Forest model \citep{breiman1996bagging, breiman2001random}, the concept of boosting and the Gradient Boosting model \cite{freund1996experiments, friedman2002stochastic} as well as the idea of Stacked Generalization \citep{wolpert1992stacked} will be outlined and evaluated. [!motivating examples: list some financial application, where ensembling brought a big benefit, e.g. saved many costs.]

The paper's structure is as follows: In the next section, the stacking and ensemble methods shall be introduced and reviewed with regards to the current state of research. Subsequently, the motivation and benefits of applying these models in the context of credit risk classification are discussed. In the following, the methodology for the practical evaluation of the models in such context is prepared.  For that, firstly the credit risk data is presented, secondly the model building process is outlined and thirdly the metrics for model evaluation are introduced. In the next section, results of the empirical analysis are presented in detail. Finally, conclusions about comparative advantages and shortfalls of the models in the context of credit risk classification are drawn and needs of further research are identified.

%The paper in which Francis Galton describes his findings on the wisdom of the crowd:
%Vox Populi. Nature 75, pages 450-451, 1907.
%http://galton.org/essays/1900-1911/galton-1907-vox-populi.pdf

\section{Literature review[3-4pages]}
Ensemble learning generally refers to the combination of multiple hypotheses in order to obtain a more powerful hypothesis. In the context of machine learning, the term \textit{hypothesis} refers to the output of an algorithm, which aims to learn a target function $sf(\mathbf{x})$ by using a set the features $\mathbf{x}$. Each algorithm that is used in the combination process of an ensemble learner is called a \textit{base learner}.

Formally this means, that given training data $D_T := \{\mathbf{x}, y\} = \{(x_1, y_1), (x_2, y_2),..., (x_N, y_N)\}$, where $x_i = (x_{i1}, x_{i2},..., x_{iK})$ is the vector of feature values for each observational unit $i \in \{1, 2,..., N\}$ and $y = f(\mathbf{x})$ is the target vector that can be modelled by an unknown function $f$ of the features. Further let $K$ denote the number of features and $N$ denote the number of observations. A set of base learners of size $M$ delivers hypotheses $h_1, h_2,..., h_M \in H$ about the true function $f$, whereby $H$ denotes the hypothesis space.

Different ways to combine the hypotheses of base learners exist. Generally, ensemble learning is most effective when combining diverse base learners. Hereby, diversity refers to error diversity, implying that the different base learners have different strengths in capturing structure in the data. \cite{brown2005diversity} showed that the combination methods of ensemble learning strategies enhance such diversity. Practical applications of ensembling techniques have shown, that stacking and ensembling models enhance diversity amongst their base learners by requiring different base learner algorithms, different hyperparameter settings, different feature subsets and different training sets \citep{online2017stacking}. [MORE RESEARCH]

In the following, different techniques to combine the hypotheses of base learners will be introduced. Furthermore, the current state of research is reviewed. 

\subsection{Bagging and the Random Forest model}
The idea of Bagging was originally proposed by \cite{breiman1996bagging}. The term abbreviates bootstrap aggregating and refers to a manipulation of the training data: Each base learner $m$ is fitted using a random sample $D^T_m$ that is uniformly drawn from the training data $D^T$. Since sampling is done with replacement, $D^T_m$ may contain duplicates of certain observational units. The hypotheses of the base learners is then aggregated by averaging in case of a regression problems or majority voting in case of a classification problem. 

In so far, bagging is a meta-algorithm that can be used with every type of base learner models. However, especially unstable base learners that are sensitive to data manipulation should be combined \citep[p.124]{breiman1996bagging}. Breiman therefore recommends to use Neural Networks, Decision Trees	or subset selection in Linear Regression. By building the base learners on different subsets of the data, the bagging procedure enhances diversity amongst them and can lead to "substantial gains in accuracy" \citep[p.123]{breiman1996bagging}.

The Random Forest model uses the bagging principle and supplements it by the random subspace approach \citep{ho1998random, breiman2001random}. This approach builds each base learner on a random sample with replacement of all available features. This implies a decorrelation of the hypotheses of the set of base learners used for ensembling. For Random Forests, Decision Trees are the preferred base learner algorithm \citep[cp.][]{breiman2001random}.

A big strength of the Random Forest is the reduction in prediction variance compared to single Decision Trees, which stems from the diversification. Clearly, the computational costs of a Random Forest can be much higher than of a single Tree, since computational time increases linearly to the number of consulted Trees. Due to the independent building of individual base learners, Random Forest model building can be speeded up by parallelization on different cores of the computer. Breiman further notes that Random Forests give "useful internal estimates of error, strength, correlation and variable importance" \citep[p.10]{breiman2001random}. Hence, even though decision rules of Random Forests are less transparent than those of single Decision Trees due to the sampling, relative variable importances can be calculated as a by-product by randomly permuting features and examining the influence on the prediction. As a single weakness, growing a large random forest can be computationally very expensive. Breiman proved that the generalization error of the ensemble converges almost surely to a limit with increasing number of Trees \citep[p.30]{breiman2001random}. In practice, the number of Trees is however restricted by the amount of available computational resources.

% Benchmark studies OR/AND Weiterentwicklungen des Random Forest??
%successfull applications:(Bauer&Kohavi, 1999; Drucker et al., 1994; Schapire, 1999).

\subsection{Boosting and the Gradient Boosting model}
Beneath Bagging, another powerful ensembling technique is Boosting. Boosting builds on the idea that the aggregation of simple base learners may lead to a strong learner. In the so-called Adaboost algorithm, Freund and Shapire (\citeyear{freund1996experiments}) start with an ensemble of one weak learner and iteratively add one more weak learner that aims to correct for the (pseudo) residuals of the current ensemble. Thereby, the calculation of these residuals is based on an iterative reweighting of the data. The weights of each datapoint $\mathbf{x_{i}}$ for model $m$ depend on the prediction accuracy of the current ensemble hypothesis for that datapoint $h^{ensemble}_{\{1,2,...,m-1\}}(\mathbf{x_{i}})$. 

With Boosting, it is possible to decrease the training error to zero \citep[p.11ff.]{freund1996experiments}. Furthermore, as long as base learners are better than random guessing, the Boosting technique is also able to reduce the generalization error independent of the base learning algorithm.

A development of the Boosting idea is the (Stochastic) Gradient Boosting model, that is currently the most commonly used Boosting model \citep{friedman2001greedy, friedman2002stochastic}. In contrast to the data weighting scheme in the Adaboost algorithm, Gradient Boosting minimizes the gradient of a loss function of the error by applying gradient descent. Typically, small Decision Trees are used as base learners of the Gradient Boosting model due to their propensity towards high prediction bias. Additionally, Gradient Boosting integrates the bagging idea. Friedman was able to show, that this integration could substantially improve accuracy and execution speed of the model \citep{friedman2002stochastic}.

The possibility to be executed fastly is a reason for the heavy use of Gradient Boosting models for machine learning problems. Furthermore, they allow to gain insights into the dependence of output and features by enabling partial dependence plots \citep[p.1219ff.]{friedman2001greedy}. However, due to its nature, Gradient Boosting models are highly prone to overfit and therefore must be accompanied with regularization methods \citep[p.1203]{friedman2002stochastic}.

% Benchmark studies OR/AND Weiterentwicklungen des Gradient Boosting??

\subsection{Stacked Generalization model}
Stacked Generalization models has been introduced by Wolpert \citeyear{wolpert1992stacked} and defines a way to combine multiple predictive algorithms by using a second-level algorithm. In contrast to Bagging or Boosting, Stacked Generalization is typically applied to a space of different base learning models. The idea is, that different kinds of models that are applied to the learning problem are able to capture only part of the problem. Combining models with different strengths in the right way would then lead to improved predictive accuracy. Stacked Generalization is therefore also referred to as a second stage model.

The Stacking algorithm involves partitioning of training dataset $D^T = (\mathbf{x}, y)$ into $M = \{1, 2,..., M\}$ disjoint parts $D^T_1, D^T_2,..., D^T_M$. On each of these subsets $D^T_m$, called "level 0 learning set", a base learner $m$, also referred to as "level 0 generalizer", is built \citep[p.57]{wolpert1992stacked}. The next step is building a meta learner ("level 1 generalizer") that produces a prediction by using the predictions of the $M$ level 0 generalizers as inputs. Due to the base learners being built on different disjoint subsets of the training data, Stacked Generalization can be seen as a sophisticated form of cross-validation. Wolpert could further show that the stacking procedure is able to reduce the bias of the base learners and thus minimizes the generalization error rate. He even recommends to use a version of Stacked Generalization in any real-world problem \citep[p.2]{wolpert1992stacked}. 

Different meta learning algorithms can be used for the combination of base learners. An optimal meta algorithm finds the best way to use the strengths of the base learners. Overfitting problem is especially present in Stacked Generalization models. This is due to the base learners all predicting the same target \citep{online2017stacking}. As a consequence, cross-validation and regularization can be used. Further more, the chosen meta learning algorithm should not be sensible to collinearity. It is therefore especially recommended to use regularized regression, gradient boosting or hill climbing methods \citep{SASpaper}. 






\section{Ensemble modelling in Credit Risk Classification: Emphasize application for finance/statistics[1-2pages]}
Techniques of Stacking and Ensemble Learning are applied to predictive problems of a broad range of topics. This paper will especially focus on the application of Ensemble Learning in credit risk classification problems. Credit risk assessment and especially the its modelling is an important part in the field of financial risk management since for most small- and medium-sized banks, interests on loans are still the primary financial source \citep[p.2]{jacobson2006internal}. The banking supervision accord Basel II, that was published in 2004 and applies to member states of the European Union since 2007, restricted the buffer capital on banks and therefore makes it especially important for them to estimate the riskiness of loan applicants \citep{basel2}. For that, banks need to be able to distinguish between applicants into risky and non-risky applicants. Two opposing factors determine the banks' business rules regarding loans: On the one hand, more loans are better. On the other hand, a bank can not afford to make to many bad loans, since this would eventually lead to a collapse of the bank. A good strategy on loans will therefore be a compromise. 

Applying Ensemble Learning techniques to credit risk modelling has already proven to be highly valuable. Zhu et al. \citeyear{zhu2017comparison} investigate credit risk assessment for small- and medium-sized chinese enterprises. For that, they carry out an experiment in which they compare the predictive performance of individual machine learning methods and ensembling methods of different complexity. They find especially the more complex ensembling methods to be of outstanding discriminative accuracy \citep[p.46f.]{zhu2017comparison}. Yu et al. \citeyear{yu2008credit} succesfully apply an ensemble learner comprising six levels of stacked Neural Networks in order to evaluate credit risk at the measurement level. Hereby, they further incorporate the Bagging approach. They conclude, that such technique "provides a promising solution to credit risk analysis" \citep[p.1443]{yu2008credit}. [MAYBE SOME MORE STUDIES]




\section{Methodology[2pages]}
In order to evaluate and compare the introduced stacking and ensemble models, an empirical evaluation study will be conducted. The code/quantlet for replication of the study can be accessed on [SOURCE]. In this section, the dataset used for the evaluation study is presented, the model building process is explained in detail and the metrics for evaluation are introduced.
\subsection{Data description}
In order to evaluate the introduced Stacking and Ensemble models, the German Credit Dataset from the UCI machine learning repository is used \citep{dataset}. The dataset classifies people as either being good or bad customers with respect to credit risk. It comprises a total number of 1000 observations and 20 features. Table [REF!] presents a summary statistic on the dataset.

\subsection{Model building process}
The model building process consists of training, tuning and testing the models. For the purpose of this study, an extensive set
of models go through this process: Beneath a Random Forest model and a Gradient Boosting model, a Decision Tree, a Neural Network as well as a Logistic Regression model are built in order to provide a diverse set of base learners for the second stage learners. Four different Stacked Generalization models are built by using different subsets of base learners' predictions, namely all versus a set of least correlated predictions, and by consulting different combiner algorithms, namely averaging and regularized logistic regression. All models deliver probabilistic predictions. 

The training process generally consists of establishing a fine-grid tuning of all relevant hyperparameters in order to find the (locally) optimal parameter choices. In order to avoid overfitting on the training dataset, each combination of hyperparameters is tested by a three-fold cross-validation process.

For the Stacked Generalization models, the Random Forest and the Gradient Boosting models are rebuilt on disjoint subsets of the training dataset. Likewise are the Decision Tree, the Neural Network and the Logistic Regression model (In total, five level 0 learners). All models are then used to predict the observations in the test dataset. Stacking model 1 is then built by averaging over the probabilistic predictions of all level 0 learners. Stacking model 2 is built by averaging over the probabilistic predictions of the three least correlated predictions of level 0 learners. Stacking model 3 is built by using again all level 0 learners and a Regularized Logistic Regression as a combining algorithm. Stacking model 4 is finally built by combining the 3 least correlated predictions of level 0 learners by using as well a Regularized Logistic Regression.

\subsection{Evaluation metrics}
In credit risk modelling, typically the misclassification costs are type-specific. This means that a false negative prediction may have different costs than a false positive prediction. When misclassification costs are known, a cost-sensitive model building strategy should be consulted. However, for the purpose of this paper misclassification costs are unknown. Furthermore, the broad field of cost-sensitive learning should may serve as a topic for other studies. Hence, equal misclassification costs for false negative and false positive predictions are assumed.

During the tuning rounds, predictive precision of the each model $m$ is evaluated on the Mean Squared Error metric (MSE) $MSE^m = \frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y}^m_i)^2$, whereby $\hat{y}^m_i$ denotes the predicted probability of model $m$ for observation $y_i$. Use of the MSE metric in the process of cross-validation ensures a minimization of the generalization bias. 

For evaluation of the models in section \ref{results}, beneath the MSE metric the Area Under Curve (AUC) is consulted. The AUC is a ranking indicator that measures the area under the receiver operating characteristic curve (ROC curve) \citep{hanley1982meaning}. For a probabilistic prediction like in the context of this study, a visualization of the ROC curve can be obtained by plotting the sensitivity against $1 -$ specificity for all cut-off thresholds between zero and one. The AUC value can therefore take values between zero and one as well. A random model would obtain an AUC value of $0.5$, which can thus function as a benchmark value in model evaluation. In a statistical sense, the AUC estimates the probability that a randomly chosen correct prediction is correctly ranked higher than a randomly chosen false prediction. 



compare metrics MSE, AUC, verbesserung zur besten baselearner
mention that no cost sensitive learning is applied.

\section{Results of empirical study[2pages]}\label{results}
In the following, this paper's results of the empirical application of Stacking and Ensemble models on credit risk classification will be presented. Table [REF!] shows the evaluations of all models on the metrics of MSE and AUC (as well as percentage of correct predictions).


\section{Conclusion[1page]}
\begin{itemize}
\item advantages of bagging models and potential problems (e.g. higher computational costs)
\item conclusions about chosen problem of analysis
\item Further research	
\end{itemize}






%\begin{table}[htb]
%        \begin{tabularx}{\textwidth}{lXXXXXX}
%            \toprule
%{\footnotesize\textbf{Variable Name}} & {\footnotesize\textbf{Decision Tree}} & {\footnotesize\textbf{Gradient Boosting}} & {\footnotesize\textbf{Random Forest}} & {\footnotesize\textbf{Logit}} & {\footnotesize\textbf{Neural Network}} & {\footnotesize\lstinline{Stacker}} \\            
%			\midrule
%{\footnotesize\lstinline{return}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} \\
%{\footnotesize\lstinline{item_price}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} \\
%{\footnotesize\lstinline{user_title}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{user_state_woe}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{item_delivered}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} \\
%{\footnotesize\lstinline{user_age}} & {\footnotesize{\checkmark}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{order_size}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{-}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} \\
%{\footnotesize\lstinline{user_order_freq}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{item_freq}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{brand_freq}} & {\footnotesize{\checkmark}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{\checkmark}} & {\footnotesize{-}} & {\footnotesize{\checkmark}} \\
%{\footnotesize\lstinline{duplicates_ordered}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{\checkmark}} & {\footnotesize{-}} & {\footnotesize{\checkmark}} \\
%{\footnotesize\lstinline{ident_except_color}} & {\footnotesize{-}} & {\footnotesize{\checkmark}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{ident_except_size}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{ident_except_sizecolor}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{\checkmark}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{user_membertime}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{item_deliveryduration}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{new_user}} & {\footnotesize{-}} & {\footnotesize{\checkmark}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{item_order_day}} & {\footnotesize{-}} & {\footnotesize{\checkmark}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{item_order_month_woe}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{\checkmark}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{item_order_day_woe}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{item_order_onwe}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{item_color_agg_woe}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%{\footnotesize\lstinline{brand_id_agg_woe}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} & {\footnotesize{\checkmark}} \\
%{\footnotesize\lstinline{item_size_agg_woe}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{-}} & {\footnotesize{\checkmark}} & {\footnotesize{-}} & {\footnotesize{-}} \\
%            \bottomrule
%     \end{tabularx}
%    \caption[Selected features]{Overview over the selected features in the models.}\label{tab:variables}
%\end{table}
%\begin{table}[htb]
%        \begin{tabularx}{\textwidth}{p{2.5cm}X>{\RaggedRight}p{1.5cm}>{\RaggedRight}p{2cm}}
%            \toprule
%{\footnotesize\textbf{Method}} & {\footnotesize\textbf{Optimal Parameters}} & {\footnotesize\textbf{AUC on test data}} & {\footnotesize\textbf{Loss value on test data}}  \\
%			\midrule
%{\footnotesize Decision Tree} & {\footnotesize\lstinline{cp = 0.00025, minsplit = 12, minbucket = 8}} & {\footnotesize\lstinline{0.68626}} & {\footnotesize\lstinline{11.48779}} \\
%{\footnotesize Gradient Boosting} & {\footnotesize\lstinline{eta = 0.45, max_depth = 6, min_child_weight = 4, nrounds = 10, lambda = 0.15, gamma = 0.5, subsample = 1}} & {\footnotesize\lstinline{0.70117}} & {\footnotesize\lstinline{11.28878}} \\
%{\footnotesize Stacking} & {\footnotesize\lstinline{no tuning}} & {\footnotesize\lstinline{0.70111}} & {\footnotesize\lstinline{11.73032}} \\
%{\footnotesize Random Forest} & {\footnotesize\lstinline{mtry = 6, sampsize = 1000, nodesize = 3}} & {\footnotesize\lstinline{0.69849}} & {\footnotesize\lstinline{11.50446}} \\
%{\footnotesize Logit} & {\footnotesize\lstinline{no tuning}} & {\footnotesize\lstinline{0.67876}} & {\footnotesize\lstinline{12.09762}}  \\
%{\footnotesize Neural Network} & {\footnotesize\lstinline{maxit = 400, decay = 0.11, size = 12}} & {\footnotesize\lstinline{0.69837}} & {\footnotesize\lstinline{11.44281}} \\
%            \bottomrule
%     \end{tabularx}
%    \caption[Results]{Overview over the performance of models.}\label{tab:results}
%\end{table}
%\newpage


%\begin{wrapfigure}{r}{0.45\textwidth}
%  \begin{center}
%    \includegraphics[width=1.\linewidth]{figures/importance_plot.png}
%  \end{center}
%\caption[Importance plot]{Importance plot of input features in Gradient Boosting model.}
%\label{fig:importance}
%\end{wrapfigure}


%\begin{figure}[ht]
%  \begin{center}
%    \includegraphics[width=1.\linewidth]{figures/pd_plots.png}
%  \end{center}
%\caption[Partial dependence plots]{Partial dependence plots of input features in Gradient Boosting model.}
%\label{fig:pdp}
%\end{figure}


%----------------------------------------------------------------------------------------
%	APPENDIX
%----------------------------------------------------------------------------------------
\appendix
\section{Appendix-part1}

\section{Appendix-part2}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
\clearpage
\bibliographystyle{apa}
\bibliography{library}
\end{document}
